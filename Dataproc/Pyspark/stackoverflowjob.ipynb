{"cells": [{"cell_type": "code", "execution_count": 1, "id": "8e1496d5-8c41-4436-a504-0bd1ac03b0a8", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": " # Import modules\nimport sys\nimport datetime\nimport re\nfrom py4j.protocol import Py4JJavaError\nfrom google.cloud import storage\n\n# Import SparkSession\nfrom pyspark.sql import SparkSession\n\n# Create Spark session\nspark = SparkSession \\\n            .builder \\\n            .appName(\"stackoverflow\") \\\n            .getOrCreate()\n\n# Variable section\nTABLE = \"bigquery-public-data.stackoverflow.posts_questions\"\nTAG = \"asp.net-mvc\"\n\nPATH = \"tmp\" + str(datetime.datetime.now())\nBUCKET_NAME = \"landing_dataset\"\nTEMP_GCS_PATH = \"gs://\" + BUCKET_NAME + \"/\" + PATH\nNEW_PATH = \"/\".join([\"stackoverflow_questions_posts.csv.gz\"])\nregex_gcs_path = \"part-[0-9a-zA-Z\\-]*.csv.gz\" \n\n# Create DataFrame from BigQuery table\ntry:\n    df = (spark.read \\\n              .format(\"bigquery\") \\\n              .option(\"table\", TABLE) \\\n              .load())\n\nexcept Py4JJavaError:\n    print(f\"{table} does not exist in BigQuery or couldn't be read.\")\n\n\ndf_filter = df.where(df.tags == TAG)\n\n# df_filter.coalesce(1).write.csv(TEMP_GCS_PATH)\n\n(df_filter\n     .coalesce(1)\n     .write\n     .options(codec = \"org.apache.hadoop.io.compress.GzipCodec\")\n     .csv(TEMP_GCS_PATH))\n\n# df_filter.show(5)\n\n\n# create storage client\nstorage_client = storage.Client()\n\n# create variable representing the temp bucket\nsource_temp_bucket = storage_client.get_bucket(BUCKET_NAME)\n\n\n# grab all the files inside bucket (_SUCCESS file + data file)\nblobs = list(source_temp_bucket.list_blobs(prefix=PATH))\n\n# for loop to copy the partition data to new location\nfor blob in blobs:\n    if re.search(regex_gcs_path, blob.name):\n        source_temp_bucket.copy_blob(blob, source_temp_bucket, NEW_PATH)\n        \n    \n# finally delete temp folder\nfor blob in blobs:\n    blob.delete()        \n        "}, {"cell_type": "code", "execution_count": 5, "id": "cf2141fe-a06c-4009-b439-274f2992ba1e", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "gs:// landing_dataset/tmp2025-10-06 09:43:44.568611\n"}], "source": "# PATH = \"tmp\" + str(datetime.datetime.now())\n# BUCKET_NAME = \" landing_dataset\"\n# TEMP_GCS_PATH = \"gs://\" + BUCKET_NAME + \"/\" + PATH\n\n# print(TEMP_GCS_PATH)"}, {"cell_type": "code", "execution_count": null, "id": "3d7fd10f-e5e0-4818-b7aa-7a1f80568592", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}
