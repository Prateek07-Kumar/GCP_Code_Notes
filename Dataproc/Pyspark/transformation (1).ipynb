{"cells": [{"cell_type": "code", "execution_count": 2, "id": "23a87380-49b4-4ca9-931a-1b9a9cf1f5df", "metadata": {}, "outputs": [{"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://my-cluster-demo1-m.us-east1-c.c.new-gcp-cloud-sql-project.internal:33311\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.3</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7fbf4ba74e20>"}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}], "source": "from pyspark.sql import SparkSession\n\nspark = (\n    SparkSession.builder\n    .master(\"local[*]\")\n    .appName(\"transformation\")\n    .getOrCreate()\n)\n\nspark\n"}, {"cell_type": "code", "execution_count": 2, "id": "074257df-c6cb-4834-b0b5-9a8fb738d83c", "metadata": {}, "outputs": [], "source": "# # define the schema\n\n# from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n\n# schema = StringType([\n#     StructField(\"emp_id\", IntegerType, True ),\n#     StructField(\"empName\", StringType, True),\n#     StructField(\"empGender\", StringType, True),\n#     StructField(\"empSalary\", FloatType, True),\n#     StructField(\"empCountry\",StringType, True)\n\n# ])"}, {"cell_type": "code", "execution_count": 1, "id": "08918350-67f3-4f03-bf7c-4581f781dfda", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-----+----------------+---------+---------+-----------+\n|empId|         empName|empGender|empSalary| empCountry|\n+-----+----------------+---------+---------+-----------+\n|    1|        John Doe|     Male|  70000.0|        USA|\n|    2|     Emily Clark|   Female|  72000.0|     Canada|\n|    3|   Michael Smith|     Male|  68000.0|         UK|\n|    4|      Sophia Lee|   Female|  75000.0|  Australia|\n|    5|      Daniel Kim|     Male|  69000.0|      India|\n|    5|      Daniel Kim|     Male|  69000.0|      India|\n|    6|    Olivia Brown|   Female|  71000.0|        USA|\n|    7|    James Wilson|     Male|  70500.0|     Canada|\n|    8|     Ava Johnson|   Female|  69500.0|    Germany|\n|    9|     Ethan Davis|     Male|  73000.0|         UK|\n|   10| Isabella Garcia|   Female|  74000.0|        USA|\n|    9|     Ethan Davis|     Male|  73000.0|         UK|\n|   10| Isabella Garcia|   Female|  74000.0|        USA|\n|   11|  Logan Martinez|     Male|  68000.0|     Mexico|\n|   12|      Mia Taylor|   Female|  72000.0|     France|\n|   13|  Lucas Anderson|     Male|  75000.0|    Ireland|\n|   14|Charlotte Thomas|   Female|  73500.0|New Zealand|\n|   15|   Jackson White|     Male|  70000.0|  Australia|\n|   14|Charlotte Thomas|   Female|  73500.0|New Zealand|\n|   15|   Jackson White|     Male|  70000.0|  Australia|\n+-----+----------------+---------+---------+-----------+\n\nroot\n |-- empId: integer (nullable = true)\n |-- empName: string (nullable = true)\n |-- empGender: string (nullable = true)\n |-- empSalary: float (nullable = true)\n |-- empCountry: string (nullable = true)\n\nRow count: 20\n"}], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n\nspark = (\n    SparkSession.builder\n    .master(\"local[*]\")\n    .appName(\"emp_schema_test\")\n    .getOrCreate()\n)\n\nemp_data = [\n    (1, \"John Doe\", \"Male\", 70000.0, \"USA\"),\n    (2, \"Emily Clark\", \"Female\", 72000.0, \"Canada\"),\n    (3, \"Michael Smith\", \"Male\", 68000.0, \"UK\"),\n    (4, \"Sophia Lee\", \"Female\", 75000.0, \"Australia\"),\n    (5, \"Daniel Kim\", \"Male\", 69000.0, \"India\"),\n    (5, \"Daniel Kim\", \"Male\", 69000.0, \"India\"),\n    (6, \"Olivia Brown\", \"Female\", 71000.0, \"USA\"),\n    (7, \"James Wilson\", \"Male\", 70500.0, \"Canada\"),\n    (8, \"Ava Johnson\", \"Female\", 69500.0, \"Germany\"),\n    (9, \"Ethan Davis\", \"Male\", 73000.0, \"UK\"),\n    (10, \"Isabella Garcia\", \"Female\", 74000.0, \"USA\"),\n    (9, \"Ethan Davis\", \"Male\", 73000.0, \"UK\"),\n    (10, \"Isabella Garcia\", \"Female\", 74000.0, \"USA\"),\n    (11, \"Logan Martinez\", \"Male\", 68000.0, \"Mexico\"),\n    (12, \"Mia Taylor\", \"Female\", 72000.0, \"France\"),\n    (13, \"Lucas Anderson\", \"Male\", 75000.0, \"Ireland\"),\n    (14, \"Charlotte Thomas\", \"Female\", 73500.0, \"New Zealand\"),\n    (15, \"Jackson White\", \"Male\", 70000.0, \"Australia\"),\n    (14, \"Charlotte Thomas\", \"Female\", 73500.0, \"New Zealand\"),\n    (15, \"Jackson White\", \"Male\", 70000.0, \"Australia\")\n]\n\n# Correct schema definition\nemp_schema = StructType([\n    StructField(\"empId\", IntegerType(), True),\n    StructField(\"empName\", StringType(), True),\n    StructField(\"empGender\", StringType(), True),\n    StructField(\"empSalary\", FloatType(), True),\n    StructField(\"empCountry\", StringType(), True)\n])\n\n# Create DataFrame with schema\ndf1 = spark.createDataFrame(emp_data, schema=emp_schema)\n\ndf1.show()\ndf1.printSchema()\nprint(\"Row count:\", df1.count())\n"}, {"cell_type": "code", "execution_count": 2, "id": "25cca55b-d4f8-497b-bcbe-8f3770cb3510", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+-------------+---------+---------+----------+------+------+\n|empId|      empName|empGender|empSalary|empCountry|Origin|   tax|\n+-----+-------------+---------+---------+----------+------+------+\n|    1|     John Doe|     Male|  70000.0|       USA| India|8400.0|\n|    2|  Emily Clark|   Female|  72000.0|    Canada| India|8640.0|\n|    3|Michael Smith|     Male|  68000.0|        UK| India|8160.0|\n|    4|   Sophia Lee|   Female|  75000.0| Australia| India|9000.0|\n|    5|   Daniel Kim|     Male|  69000.0|     India| India|8280.0|\n+-----+-------------+---------+---------+----------+------+------+\nonly showing top 5 rows\n\n"}], "source": "# withColumn() : to add columns\n# add two more columns\n      #country ==> constant column ==> India\n      #tax => derived column ==> 12%(salary)\n\nfrom pyspark.sql.functions import lit\n        \ndf2 = df1.withColumn(\"Origin\", lit(\"India\")).withColumn(\"tax\",df1.empSalary*0.12)\n\ndf2.show(5)\n"}, {"cell_type": "code", "execution_count": 3, "id": "fc442576-de91-4d8b-95d0-b763f066a677", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+-------------+---------+---------+----------+---------+------+\n|empId|      empName|empGender|empSalary|empCountry|empOrigin|empTax|\n+-----+-------------+---------+---------+----------+---------+------+\n|    1|     John Doe|     Male|  70000.0|       USA|    India|8400.0|\n|    2|  Emily Clark|   Female|  72000.0|    Canada|    India|8640.0|\n|    3|Michael Smith|     Male|  68000.0|        UK|    India|8160.0|\n|    4|   Sophia Lee|   Female|  75000.0| Australia|    India|9000.0|\n|    5|   Daniel Kim|     Male|  69000.0|     India|    India|8280.0|\n+-----+-------------+---------+---------+----------+---------+------+\nonly showing top 5 rows\n\n"}], "source": "# withColumnRenamed()  : to rename the columns\n    #two columns => country, tax\n    \ndf3 = df2.withColumnRenamed(\"Origin\", \"empOrigin\") \\\n        .withColumnRenamed(\"tax\", \"empTax\")\n\ndf3.show(5)\n# df3.printSchema() #data type"}, {"cell_type": "code", "execution_count": 4, "id": "7bd99ba8-7b23-4208-b80e-b11fab8ba84b", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+-------------+---------+\n|empId|      empName|empGender|\n+-----+-------------+---------+\n|    1|     John Doe|     Male|\n|    2|  Emily Clark|   Female|\n|    3|Michael Smith|     Male|\n|    4|   Sophia Lee|   Female|\n|    5|   Daniel Kim|     Male|\n+-----+-------------+---------+\nonly showing top 5 rows\n\n"}], "source": "# way to select the specific columns\nfrom pyspark.sql.functions import col\n\n# df3.select(\"empId\", \"empName\", col(\"empGender\")).show(5) #both are same\n\ndf3.select(\"empId\", df3.empName, col(\"empGender\")).show(5)\n"}, {"cell_type": "code", "execution_count": 5, "id": "ca7ab7f5-2c71-4504-a674-1109ddf5e909", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+-------------+---------+---------+----------+---------+------+\n|empId|      empName|empGender|empSalary|empCountry|empOrigin|empTax|\n+-----+-------------+---------+---------+----------+---------+------+\n|    1|     John Doe|     Male|  70000.0|       USA|    India|8400.0|\n|    2|  Emily Clark|   Female|  72000.0|    Canada|    India|8640.0|\n|    3|Michael Smith|     Male|  68000.0|        UK|    India|8160.0|\n|    4|   Sophia Lee|   Female|  75000.0| Australia|    India|9000.0|\n|    5|   Daniel Kim|     Male|  69000.0|     India|    India|8280.0|\n+-----+-------------+---------+---------+----------+---------+------+\nonly showing top 5 rows\n\n"}], "source": "df3.show(5)"}, {"cell_type": "code", "execution_count": 6, "id": "3565466e-0d15-4792-8cf5-a0c0175255b1", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+-------------+---------+---------+----------+---------+------+\n|empId|      empName|empGender|empSalary|empCountry|empOrigin|empTax|\n+-----+-------------+---------+---------+----------+---------+------+\n|    1|     John Doe|     Male|  70000.0|       USA|    India|8400.0|\n|    2|  Emily Clark|   Female|  72000.0|    Canada|    India|8640.0|\n|    3|Michael Smith|     Male|  68000.0|        UK|    India|8160.0|\n|    4|   Sophia Lee|   Female|  75000.0| Australia|    India|9000.0|\n|    5|   Daniel Kim|     Male|  69000.0|     India|    India|8280.0|\n+-----+-------------+---------+---------+----------+---------+------+\nonly showing top 5 rows\n\n"}], "source": "# case conditions :\n\n    #male ==> m\n    #female ==> f\n\n# df4 = df3.select(\"empId\", \n#                  \"empName\", \n#                  (\"empGender\"), \n#                  \"empSalary\", \n#                  \"empCountry\", \n#                  \"empOrigin\", \n#                  \"empTax\")\n\n# df4.show(5)"}, {"cell_type": "code", "execution_count": 7, "id": "ca5bee6b-4e36-4ddf-9589-91d4336b315b", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+-------------+---------+---------+----------+---------+------+\n|empId|      empName|empGender|empSalary|empCountry|empOrigin|empTax|\n+-----+-------------+---------+---------+----------+---------+------+\n|    1|     John Doe|        M|  70000.0|       USA|    India|8400.0|\n|    2|  Emily Clark|        F|  72000.0|    Canada|    India|8640.0|\n|    3|Michael Smith|        M|  68000.0|        UK|    India|8160.0|\n|    4|   Sophia Lee|        F|  75000.0| Australia|    India|9000.0|\n|    5|   Daniel Kim|        M|  69000.0|     India|    India|8280.0|\n+-----+-------------+---------+---------+----------+---------+------+\nonly showing top 5 rows\n\n"}], "source": "# case conditions :\n\n    #male ==> m\n    #female ==> f\n    \nfrom pyspark.sql.functions import when\n    \ndf4 = df3.select(\"empId\", \n                 \"empName\", \n                 when(df3.empGender == 'Male', 'M').otherwise('F').alias(\"empGender\"), \n                 \"empSalary\", \n                 \"empCountry\", \n                 \"empOrigin\", \n                 \"empTax\")\n\ndf4.show(5)"}, {"cell_type": "code", "execution_count": 8, "id": "596e7a3a-cf8c-49e2-bcaf-822b28715e8e", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+--------------+---------+---------+----------+---------+------+\n|empId|       empName|empGender|empSalary|empCountry|empOrigin|empTax|\n+-----+--------------+---------+---------+----------+---------+------+\n|   11|Logan Martinez|        M|  68000.0|    Mexico|    India|8160.0|\n|    3| Michael Smith|        M|  68000.0|        UK|    India|8160.0|\n|    5|    Daniel Kim|        M|  69000.0|     India|    India|8280.0|\n|    5|    Daniel Kim|        M|  69000.0|     India|    India|8280.0|\n|    8|   Ava Johnson|        F|  69500.0|   Germany|    India|8340.0|\n+-----+--------------+---------+---------+----------+---------+------+\nonly showing top 5 rows\n\n"}], "source": "# orderBy() or sort() : to sort the data\n\n# df4.orderBy(df4.empSalary.desc()).show(5)\n\n# both the statement are same orderby or sort give you same ans\n\n# use desc or asc order in both statement\n\ndf4.sort(df4.empSalary).show(5) \n"}, {"cell_type": "code", "execution_count": 9, "id": "803175fd-0201-41d2-84c9-ded7aa5aade3", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+-------------+---------+---------+----------+---------+------+\n|empId|      empName|empGender|empSalary|empCountry|empOrigin|empTax|\n+-----+-------------+---------+---------+----------+---------+------+\n|    1|     John Doe|        M|  70000.0|       USA|    India|8400.0|\n|    2|  Emily Clark|        F|  72000.0|    Canada|    India|8640.0|\n|    3|Michael Smith|        M|  68000.0|        UK|    India|8160.0|\n|    4|   Sophia Lee|        F|  75000.0| Australia|    India|9000.0|\n|    5|   Daniel Kim|        M|  69000.0|     India|    India|8280.0|\n+-----+-------------+---------+---------+----------+---------+------+\nonly showing top 5 rows\n\n"}], "source": "df4.show(5)"}, {"cell_type": "code", "execution_count": 10, "id": "62ccf58f-948a-4334-bc44-8d1d3277b353", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-----+----------------+---------+---------+-----------+---------+------+\n|empId|         empName|empGender|empSalary| empCountry|empOrigin|empTax|\n+-----+----------------+---------+---------+-----------+---------+------+\n|    1|        John Doe|        M|  70000.0|        USA|    India|8400.0|\n|    2|     Emily Clark|        F|  72000.0|     Canada|    India|8640.0|\n|    3|   Michael Smith|        M|  68000.0|         UK|    India|8160.0|\n|    4|      Sophia Lee|        F|  75000.0|  Australia|    India|9000.0|\n|    5|      Daniel Kim|        M|  69000.0|      India|    India|8280.0|\n|    6|    Olivia Brown|        F|  71000.0|        USA|    India|8520.0|\n|    7|    James Wilson|        M|  70500.0|     Canada|    India|8460.0|\n|    8|     Ava Johnson|        F|  69500.0|    Germany|    India|8340.0|\n|    9|     Ethan Davis|        M|  73000.0|         UK|    India|8760.0|\n|   10| Isabella Garcia|        F|  74000.0|        USA|    India|8880.0|\n|   11|  Logan Martinez|        M|  68000.0|     Mexico|    India|8160.0|\n|   12|      Mia Taylor|        F|  72000.0|     France|    India|8640.0|\n|   13|  Lucas Anderson|        M|  75000.0|    Ireland|    India|9000.0|\n|   14|Charlotte Thomas|        F|  73500.0|New Zealand|    India|8820.0|\n|   15|   Jackson White|        M|  70000.0|  Australia|    India|8400.0|\n+-----+----------------+---------+---------+-----------+---------+------+\n\n"}, {"data": {"text/plain": "15"}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": "# dropDuplicates() : to remove the duplicates\n\n# df4.dropDuplicates().show()\ndf4.dropDuplicates().orderBy(\"empId\").show()\ndf4.dropDuplicates().count()"}, {"cell_type": "code", "execution_count": 11, "id": "607585fd-0040-49d7-bedd-defe13ecc7bd", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+----------------+---------+---------+-----------+---------+------+\n|empId|         empName|empGender|empSalary| empCountry|empOrigin|empTax|\n+-----+----------------+---------+---------+-----------+---------+------+\n|    1|        John Doe|        M|  70000.0|        USA|    India|8400.0|\n|    2|     Emily Clark|        F|  72000.0|     Canada|    India|8640.0|\n|    3|   Michael Smith|        M|  68000.0|         UK|    India|8160.0|\n|    4|      Sophia Lee|        F|  75000.0|  Australia|    India|9000.0|\n|    5|      Daniel Kim|        M|  69000.0|      India|    India|8280.0|\n|    5|      Daniel Kim|        M|  69000.0|      India|    India|8280.0|\n|    6|    Olivia Brown|        F|  71000.0|        USA|    India|8520.0|\n|    7|    James Wilson|        M|  70500.0|     Canada|    India|8460.0|\n|    8|     Ava Johnson|        F|  69500.0|    Germany|    India|8340.0|\n|    9|     Ethan Davis|        M|  73000.0|         UK|    India|8760.0|\n|   10| Isabella Garcia|        F|  74000.0|        USA|    India|8880.0|\n|    9|     Ethan Davis|        M|  73000.0|         UK|    India|8760.0|\n|   10| Isabella Garcia|        F|  74000.0|        USA|    India|8880.0|\n|   11|  Logan Martinez|        M|  68000.0|     Mexico|    India|8160.0|\n|   12|      Mia Taylor|        F|  72000.0|     France|    India|8640.0|\n|   13|  Lucas Anderson|        M|  75000.0|    Ireland|    India|9000.0|\n|   14|Charlotte Thomas|        F|  73500.0|New Zealand|    India|8820.0|\n|   15|   Jackson White|        M|  70000.0|  Australia|    India|8400.0|\n|   14|Charlotte Thomas|        F|  73500.0|New Zealand|    India|8820.0|\n|   15|   Jackson White|        M|  70000.0|  Australia|    India|8400.0|\n+-----+----------------+---------+---------+-----------+---------+------+\n\n"}], "source": "df4.show()"}, {"cell_type": "code", "execution_count": 12, "id": "4c032f64-b39b-4894-bede-41d6b9d7fcd9", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+----------------+---------+---------+-----------+---------+------+\n|empId|         empName|empGender|empSalary| empCountry|empOrigin|empTax|\n+-----+----------------+---------+---------+-----------+---------+------+\n|    1|        John Doe|        M|  70000.0|        USA|    India|8400.0|\n|    2|     Emily Clark|        F|  72000.0|     Canada|    India|8640.0|\n|    3|   Michael Smith|        M|  68000.0|         UK|    India|8160.0|\n|    4|      Sophia Lee|        F|  75000.0|  Australia|    India|9000.0|\n|    5|      Daniel Kim|        M|  69000.0|      India|    India|8280.0|\n|    6|    Olivia Brown|        F|  71000.0|        USA|    India|8520.0|\n|    7|    James Wilson|        M|  70500.0|     Canada|    India|8460.0|\n|    8|     Ava Johnson|        F|  69500.0|    Germany|    India|8340.0|\n|    9|     Ethan Davis|        M|  73000.0|         UK|    India|8760.0|\n|   10| Isabella Garcia|        F|  74000.0|        USA|    India|8880.0|\n|   11|  Logan Martinez|        M|  68000.0|     Mexico|    India|8160.0|\n|   12|      Mia Taylor|        F|  72000.0|     France|    India|8640.0|\n|   13|  Lucas Anderson|        M|  75000.0|    Ireland|    India|9000.0|\n|   14|Charlotte Thomas|        F|  73500.0|New Zealand|    India|8820.0|\n|   15|   Jackson White|        M|  70000.0|  Australia|    India|8400.0|\n+-----+----------------+---------+---------+-----------+---------+------+\n\n"}], "source": "# distinct() : to display the unique records\n\n# df4.distinct().show()\n\ndf4.distinct().orderBy(\"empId\").show()"}, {"cell_type": "code", "execution_count": 13, "id": "087fd209-75b3-42f8-b4cc-d5ad2d1e59cd", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+-----------+---------+---------+----------+---------+------+\n|empId|    empName|empGender|empSalary|empCountry|empOrigin|empTax|\n+-----+-----------+---------+---------+----------+---------+------+\n|    8|Ava Johnson|        F|  69500.0|   Germany|    India|8340.0|\n+-----+-----------+---------+---------+----------+---------+------+\n\n"}], "source": "# where() or filter() : to filter the data \n\ndf4.where((df4.empSalary > 55000) & (df4.empGender=='F') & (df4.empName.like(\"A%\"))).show()"}, {"cell_type": "code", "execution_count": 14, "id": "2dcecf5a-0f7e-4ad9-bcc7-f468829e5366", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+----------------+---------+---------+-----------+---------+------+\n|empId|         empName|empGender|empSalary| empCountry|empOrigin|empTax|\n+-----+----------------+---------+---------+-----------+---------+------+\n|    8|     Ava Johnson|        F|  69500.0|    Germany|    India|8340.0|\n|    6|    Olivia Brown|        F|  71000.0|        USA|    India|8520.0|\n|    2|     Emily Clark|        F|  72000.0|     Canada|    India|8640.0|\n|   12|      Mia Taylor|        F|  72000.0|     France|    India|8640.0|\n|   14|Charlotte Thomas|        F|  73500.0|New Zealand|    India|8820.0|\n|   14|Charlotte Thomas|        F|  73500.0|New Zealand|    India|8820.0|\n|   10| Isabella Garcia|        F|  74000.0|        USA|    India|8880.0|\n|   10| Isabella Garcia|        F|  74000.0|        USA|    India|8880.0|\n|    4|      Sophia Lee|        F|  75000.0|  Australia|    India|9000.0|\n+-----+----------------+---------+---------+-----------+---------+------+\n\n"}], "source": "df4.where(\n          (df4.empSalary > 55000) & \n          (df4.empGender=='F') |\n          (df4.empName.like(\"A%\"))) \\\n          .orderBy(\"empSalary\") \\\n          .show()"}, {"cell_type": "code", "execution_count": 16, "id": "77da791a-036a-44b0-bb3f-60c15a77ae1d", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+----------------+---------+---------+-----------+---------+------+\n|empId|         empName|empGender|empSalary| empCountry|empOrigin|empTax|\n+-----+----------------+---------+---------+-----------+---------+------+\n|    8|     Ava Johnson|        F|  69500.0|    Germany|    India|8340.0|\n|    6|    Olivia Brown|        F|  71000.0|        USA|    India|8520.0|\n|    2|     Emily Clark|        F|  72000.0|     Canada|    India|8640.0|\n|   12|      Mia Taylor|        F|  72000.0|     France|    India|8640.0|\n|   14|Charlotte Thomas|        F|  73500.0|New Zealand|    India|8820.0|\n|   14|Charlotte Thomas|        F|  73500.0|New Zealand|    India|8820.0|\n|   10| Isabella Garcia|        F|  74000.0|        USA|    India|8880.0|\n|   10| Isabella Garcia|        F|  74000.0|        USA|    India|8880.0|\n|    4|      Sophia Lee|        F|  75000.0|  Australia|    India|9000.0|\n+-----+----------------+---------+---------+-----------+---------+------+\n\n"}], "source": "df4.where((df4.empSalary > 55000) & (df4.empGender=='F') | (df4.empName.like(\"A%\"))).orderBy(\"empSalary\").show()"}, {"cell_type": "code", "execution_count": 17, "id": "9800c5e4-7509-4cfc-a178-58f3c422d067", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+-------+---------+---------+-------------+\n|empId|empName|empGender|empSalary|empDepartment|\n+-----+-------+---------+---------+-------------+\n|    1|  Aarav|        M|    75000|  Engineering|\n|    2|   Diya|        F|    68000|    Marketing|\n|    3|  Kabir|        M|    72000|      Finance|\n|    4|  Meera|        F|    81000|  Engineering|\n|    5|  Rohan|        M|    60000|           HR|\n|    6|   Isha|        F|    90000|        Sales|\n|    7|  Arjun|        M|    78000|  Engineering|\n|    8|  Anaya|        F|    67000|    Marketing|\n|    9| Vivaan|        M|    71000|      Finance|\n|   10| Saanvi|        F|    85000|        Sales|\n|   11|  Yuvan|        M|    62000|           HR|\n|   12|  Kiara|        F|    76000|  Engineering|\n+-----+-------+---------+---------+-------------+\n\nroot\n |-- empId: integer (nullable = true)\n |-- empName: string (nullable = true)\n |-- empGender: string (nullable = true)\n |-- empSalary: integer (nullable = true)\n |-- empDepartment: string (nullable = true)\n\n"}], "source": "data = [\n    (1, \"Aarav\", \"M\", 75000, \"Engineering\"),\n    (2, \"Diya\", \"F\", 68000, \"Marketing\"),\n    (3, \"Kabir\", \"M\", 72000, \"Finance\"),\n    (4, \"Meera\", \"F\", 81000, \"Engineering\"),\n    (5, \"Rohan\", \"M\", 60000, \"HR\"),\n    (6, \"Isha\", \"F\", 90000, \"Sales\"),\n    (7, \"Arjun\", \"M\", 78000, \"Engineering\"),\n    (8, \"Anaya\", \"F\", 67000, \"Marketing\"),\n    (9, \"Vivaan\", \"M\", 71000, \"Finance\"),\n    (10, \"Saanvi\", \"F\", 85000, \"Sales\"),\n    (11, \"Yuvan\", \"M\", 62000, \"HR\"),\n    (12, \"Kiara\", \"F\", 76000, \"Engineering\")\n    ]\n\n# Define schema using instances of types\nschema = StructType([\n    StructField(\"empId\", IntegerType(), True),\n    StructField(\"empName\", StringType(), True),\n    StructField(\"empGender\", StringType(), True),\n    StructField(\"empSalary\", IntegerType(), True),\n    StructField(\"empDepartment\", StringType(), True),\n])\n\n\ndf = spark.createDataFrame(data, schema)\ndf.show()\ndf.printSchema()\n"}, {"cell_type": "code", "execution_count": 18, "id": "5813027b-c878-4d31-abd8-db53c706ab1f", "metadata": {}, "outputs": [{"data": {"text/plain": "12"}, "execution_count": 18, "metadata": {}, "output_type": "execute_result"}], "source": "# count() : to get the no. of records\n\ndf.count()"}, {"cell_type": "code", "execution_count": 24, "id": "a0252d4d-2b7d-4e97-8de2-b6aa532409d6", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------+\n|empCount|\n+--------+\n|      12|\n+--------+\n\n+---------+\n|maxSalary|\n+---------+\n|    90000|\n+---------+\n\n+---------+\n|minSalary|\n+---------+\n|    60000|\n+---------+\n\n+---------+\n|avgSalary|\n+---------+\n|  73750.0|\n+---------+\n\n+---------+\n|sumSalary|\n+---------+\n|   885000|\n+---------+\n\n"}], "source": "# aggregate function : count, max, min, sum, avg\n\nfrom pyspark.sql.functions import max, min, sum, avg,count\n\n# df.groupby(\"empDepartment\").agg(count(\"*\")).show()\n\n# using alias \n\ndf.agg(count(\"*\").alias(\"empCount\")).show()\ndf.agg(max(\"empSalary\").alias(\"maxSalary\")).show()\ndf.agg(min(\"empSalary\").alias(\"minSalary\")).show()\ndf.agg(avg(\"empSalary\").alias(\"avgSalary\")).show()\ndf.agg(sum(\"empSalary\").alias(\"sumSalary\")).show()\n\n# df.groupby(\"empDepartment\").agg(count(\"*\").alias(\"empCount\")).show()\n\n# df.groupby(\"empDepartment\").agg(count(\"*\").alias(\"empCount\")).agg(max(\"empSalary\").show()"}, {"cell_type": "code", "execution_count": null, "id": "be8c4d94-9449-433c-8a08-a756db788785", "metadata": {}, "outputs": [], "source": "# df.groupby(\"empDepartment\").agg(count(\"*\").alias(\"empCount\"))\n#                                           .alias"}, {"cell_type": "code", "execution_count": 25, "id": "8759367b-fcb3-4b4a-a290-14a11ef31589", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------------+--------+---------+---------+---------+---------+\n|empDepartment|empCount|maxSalary|minSalary|avgSalary|sumSalary|\n+-------------+--------+---------+---------+---------+---------+\n|        Sales|       2|    90000|    85000|  87500.0|   175000|\n|  Engineering|       4|    81000|    75000|  77500.0|   310000|\n|           HR|       2|    62000|    60000|  61000.0|   122000|\n|      Finance|       2|    72000|    71000|  71500.0|   143000|\n|    Marketing|       2|    68000|    67000|  67500.0|   135000|\n+-------------+--------+---------+---------+---------+---------+\n\n"}], "source": "# find the emp count for each department\n\ndf.groupby(\"empDepartment\").agg(count(\"*\").alias(\"empCount\"), \\\n                            max(\"empSalary\").alias(\"maxSalary\"), \\\n                            min(\"empSalary\").alias(\"minSalary\"), \\\n                            avg(\"empSalary\").alias(\"avgSalary\"), \\\n                            sum(\"empSalary\").alias(\"sumSalary\")).show()"}, {"cell_type": "code", "execution_count": 26, "id": "71e7d8e2-e4e6-4c47-9b6c-78e8fd55aaed", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+-------+---+\n| id|   name|age|\n+---+-------+---+\n|  1|   Anil| 27|\n|  2|sandeep| 20|\n|  3|   riya| 29|\n+---+-------+---+\n\n+---+----+---+\n| id|name|age|\n+---+----+---+\n|  3|riya| 20|\n|  4|rani| 26|\n+---+----+---+\n\n+---+----+---+\n| id|name|age|\n+---+----+---+\n|  5|liya| 29|\n|  6|mani| 26|\n+---+----+---+\n\n"}], "source": "# union / union all : to merge the data\n\n# condition :\n    # schema should be same \n    # sequence should be same\n\ndata1 = [(1, 'Anil',27),\n        (2, 'sandeep', 20), #jan\n        (3, 'riya', 29)]\nschema1 = ['id', 'name', 'age']\n\ndata2 = [(3, 'riya', 20),\n        (4, 'rani', 26)]   #feb  \nschema2 = ['id', 'name', 'age']\n\ndata3 = [(5, 'liya',29),\n        (6, 'mani', 26)]  #march\nschema3 = ['id', 'name', 'age']\n\ndf1 = spark.createDataFrame(data1, schema1)\ndf2 = spark.createDataFrame(data2, schema2)\ndf3 = spark.createDataFrame(data3, schema3)\n\n\ndf1.show()\ndf2.show()\ndf3.show()\n"}, {"cell_type": "code", "execution_count": 29, "id": "ae563915-2a3c-484b-b5e8-c8106cd8c908", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+-------+---+\n| id|   name|age|\n+---+-------+---+\n|  1|   Anil| 27|\n|  2|sandeep| 20|\n|  3|   riya| 20|\n|  3|   riya| 29|\n|  4|   rani| 26|\n|  5|   liya| 29|\n|  6|   mani| 26|\n+---+-------+---+\n\n"}], "source": "df_union = df1.union(df2).union(df3)\n# df_union.show()  #table show the duplicate value also use union all\ndf_union.dropDuplicates().sort(\"id\").show()"}, {"cell_type": "code", "execution_count": 43, "id": "afaa5523-5c04-47a1-a66c-8bfab00a1853", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "emp_df :\n+-----+-------+------+---------+------+\n|empId|empName|deptId|empSalary|cityId|\n+-----+-------+------+---------+------+\n|    1|  Aarav|     1|    70000|     1|\n|    2| Vivaan|     2|    72000|     2|\n|    3|Reyansh|     3|    68000|     2|\n|    2| Aadhya|     2|    75000|     2|\n|    3| Ishaan|     3|    71000|     3|\n|    4|   Diya|     4|    73000|     4|\n|    5|  Anaya|  null|    76000|  null|\n|    6|  Arjun|     4|    69000|     3|\n|    7|   Myra|     2|    74000|     1|\n|    8|  Yuvan|  null|    67000|     4|\n|    9|  Kiara|     2|    71000|     3|\n|    7|Shaurya|     3|    70000|  null|\n|    8|  Meera|     1|    72000|     4|\n|   10|  Rohan|     1|    73000|  null|\n|   10| Saanvi|     1|    76000|     4|\n+-----+-------+------+---------+------+\n\ndept_df :\n+------+--------+\n|deptId|deptName|\n+------+--------+\n|     1|      HR|\n|     2|      IT|\n|     3|   Sales|\n|     4| Finance|\n+------+--------+\n\naddress_df :\n"}, {"ename": "NameError", "evalue": "name 'address_df' is not defined", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", "Cell \u001b[0;32mIn[43], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maddress_df :\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m emp_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(address_data,add_schema)\n\u001b[0;32m---> 55\u001b[0m \u001b[43maddress_df\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# print(\"emp_df :\")\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# emp_df = spark.createDataFrame(emp_data,emp_schema)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# emp_df.show()\u001b[39;00m\n", "\u001b[0;31mNameError\u001b[0m: name 'address_df' is not defined"]}], "source": "# joins : inner : left : right, full, cross\n\n\nemp_data = [\n    (1,  \"Aarav\",    1,   70000,  1),\n    (2,  \"Vivaan\",   2,   72000,  2),\n    (3,  \"Reyansh\",  3,   68000,  2),\n    (2,  \"Aadhya\",   2,   75000,  2),\n    (3,  \"Ishaan\",   3,   71000,  3),\n    (4,  \"Diya\",     4,   73000,  4),\n    (5,  \"Anaya\",    None,   76000,  None),\n    (6,  \"Arjun\",    4,   69000,  3),\n    (7,  \"Myra\",     2,   74000,  1),\n    (8, \"Yuvan\",   None,   67000, 4),\n    (9, \"Kiara\",    2,   71000,  3),\n    (7, \"Shaurya\",  3,   70000,  None),\n    (8, \"Meera\",    1,   72000,  4),\n    (10, \"Rohan\",    1,   73000,  None),\n    (10, \"Saanvi\",   1,   76000,  4)\n]\n\nemp_schema = [\"empId\", \"empName\", \"deptId\", \"empSalary\", \"cityId\"]\n\n\ndept_data = [\n    (1, \"HR\"),\n    (2, \"IT\"),\n    (3, \"Sales\"),\n    (4, \"Finance\")\n]\n\ndept_schema = [\"deptId\", \"deptName\"]\n\naddress_data = [\n    (1, \"hyd\"),\n    (2, \"blr\"),\n    (3, \"chn\"),\n    (4, \"kkt\")\n]\n\nadd_schema = [\"cityId\", \"cityName\"]\n\nprint(\"emp_df :\")\nemp_df = spark.createDataFrame(emp_data,emp_schema)\nemp_df.show()\n\n\nprint(\"dept_df :\")\nemp_df = spark.createDataFrame(dept_data,dept_schema)\nemp_df.show()\n\n\nprint(\"address_df :\")\nemp_df = spark.createDataFrame(address_data,add_schema)\naddress_df.show()\n\n\n\n\n# print(\"emp_df :\")\n# emp_df = spark.createDataFrame(emp_data,emp_schema)\n# emp_df.show()\n\n"}, {"cell_type": "code", "execution_count": 44, "id": "ba582e5b-d37c-42c5-bc02-2a65a144e9d4", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "emp_df:\n+-----+-------+------+---------+------+\n|empId|empName|deptId|empSalary|cityId|\n+-----+-------+------+---------+------+\n|    1|  Aarav|     1|    70000|     1|\n|    2| Vivaan|     2|    72000|     2|\n|    3|Reyansh|     3|    68000|     2|\n|    2| Aadhya|     2|    75000|     2|\n|    3| Ishaan|     3|    71000|     3|\n|    4|   Diya|     4|    73000|     4|\n|    5|  Anaya|  null|    76000|  null|\n|    6|  Arjun|     4|    69000|     3|\n|    7|   Myra|     2|    74000|     1|\n|    8|  Yuvan|  null|    67000|     4|\n|    9|  Kiara|     2|    71000|     3|\n|    7|Shaurya|     3|    70000|  null|\n|    8|  Meera|     1|    72000|     4|\n|   10|  Rohan|     1|    73000|  null|\n|   10| Saanvi|     1|    76000|     4|\n+-----+-------+------+---------+------+\n\ndept_df:\n+------+--------+\n|deptId|deptName|\n+------+--------+\n|     1|      HR|\n|     2|      IT|\n|     3|   Sales|\n|     4| Finance|\n+------+--------+\n\naddress_df:\n+------+--------+\n|cityId|cityName|\n+------+--------+\n|     1|     hyd|\n|     2|     blr|\n|     3|     chn|\n|     4|     kkt|\n+------+--------+\n\n"}], "source": "emp_data = [\n    (1,  \"Aarav\",    1,   70000,  1),\n    (2,  \"Vivaan\",   2,   72000,  2),\n    (3,  \"Reyansh\",  3,   68000,  2),\n    (2,  \"Aadhya\",   2,   75000,  2),  # Duplicate empId 2\n    (3,  \"Ishaan\",   3,   71000,  3),  # Duplicate empId 3\n    (4,  \"Diya\",     4,   73000,  4),\n    (5,  \"Anaya\",    None, 76000,  None),\n    (6,  \"Arjun\",    4,   69000,  3),\n    (7,  \"Myra\",     2,   74000,  1),\n    (8,  \"Yuvan\",    None, 67000,  4),\n    (9,  \"Kiara\",    2,   71000,  3),\n    (7,  \"Shaurya\",  3,   70000,  None),  # Duplicate empId 7\n    (8,  \"Meera\",    1,   72000,  4),     # Duplicate empId 8\n    (10, \"Rohan\",    1,   73000,  None),  # Duplicate empId 10\n    (10, \"Saanvi\",   1,   76000,  4)      # Duplicate empId 10\n]\nemp_schema = [\"empId\", \"empName\", \"deptId\", \"empSalary\", \"cityId\"]\n\ndept_data = [\n    (1, \"HR\"),\n    (2, \"IT\"),\n    (3, \"Sales\"),\n    (4, \"Finance\")\n]\ndept_schema = [\"deptId\", \"deptName\"]\n\naddress_data = [\n    (1, \"hyd\"),\n    (2, \"blr\"),\n    (3, \"chn\"),\n    (4, \"kkt\")\n]\nadd_schema = [\"cityId\", \"cityName\"]\n\n# Create DataFrames with CORRECT variable names\nprint(\"emp_df:\")\nemp_df = spark.createDataFrame(emp_data, emp_schema)\nemp_df.show()\n\nprint(\"dept_df:\")\ndept_df = spark.createDataFrame(dept_data, dept_schema)  # Fixed variable name\ndept_df.show()\n\nprint(\"address_df:\")\naddress_df = spark.createDataFrame(address_data, add_schema)  # Fixed variable name\naddress_df.show()"}, {"cell_type": "code", "execution_count": 61, "id": "77e363b4-54c3-41a7-8932-cedf87e4a826", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+-------+------+---------+------+--------+--------+\n|empId|empName|deptId|empSalary|cityId|deptName|cityName|\n+-----+-------+------+---------+------+--------+--------+\n|    7|   Myra|     2|    74000|     1|      IT|     hyd|\n|    1|  Aarav|     1|    70000|     1|      HR|     hyd|\n|    6|  Arjun|     4|    69000|     3| Finance|     chn|\n|    9|  Kiara|     2|    71000|     3|      IT|     chn|\n|    3| Ishaan|     3|    71000|     3|   Sales|     chn|\n|    2| Aadhya|     2|    75000|     2|      IT|     blr|\n|    2| Vivaan|     2|    72000|     2|      IT|     blr|\n|    3|Reyansh|     3|    68000|     2|   Sales|     blr|\n|    4|   Diya|     4|    73000|     4| Finance|     kkt|\n|   10| Saanvi|     1|    76000|     4|      HR|     kkt|\n|    8|  Meera|     1|    72000|     4|      HR|     kkt|\n+-----+-------+------+---------+------+--------+--------+\n\n"}], "source": "# You can drop the deptId & cityId column\ninner_df = emp_df.join(dept_df, emp_df.deptId == dept_df.deptId, \"inner\") \\\n                 .join(address_df, emp_df.cityId == address_df.cityId, \"inner\") \\\n                 .drop(dept_df.deptId) \\ #you can drop the deptId & cityId column\n                 .drop(address_df.cityId) \\\n                 .dropDuplicates() # Uncomment to drop duplicates\n\ninner_df.show()"}, {"cell_type": "code", "execution_count": 63, "id": "d721dec4-bed8-4b74-bc7c-21b4519b31c6", "metadata": {}, "outputs": [{"ename": "SyntaxError", "evalue": "unexpected character after line continuation character (1535683249.py, line 3)", "output_type": "error", "traceback": ["\u001b[0;36m  Cell \u001b[0;32mIn[63], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    .drop(emp_df.deptId) \\ #you can drop the deptId & cityId column\u001b[0m\n\u001b[0m                                                                   \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"]}], "source": "# left_df = emp_df.join(dept_df, emp_df.deptId == dept_df.deptId, \"left\") \\\n#                  .join(address_df, emp_df.cityId == address_df.cityId,\"left\") \\\n#                  .drop(emp_df.deptId) \\ #you can drop the deptId & cityId column\n#                  .drop(emp_df.cityId) \\\n#                  .dropDuplicates() \\  #drop the duplicate value\n#                  .dropna()\n# #                  .fillna(\"unknown\") #to fill null value\n# # left_df.show()\n# left_df.orderBy(\"empId\").show()"}, {"cell_type": "code", "execution_count": 66, "id": "4dfd0c78-e75b-475f-a91e-ebd516522168", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+-------+---------+------+--------+------+--------+\n|empId|empName|empSalary|deptId|deptName|cityId|cityName|\n+-----+-------+---------+------+--------+------+--------+\n|    1|  Aarav|    70000|     1|      HR|     1|     hyd|\n|    2| Aadhya|    75000|     2|      IT|     2|     blr|\n|    2| Vivaan|    72000|     2|      IT|     2|     blr|\n|    3|Reyansh|    68000|     3|   Sales|     2|     blr|\n|    3| Ishaan|    71000|     3|   Sales|     3|     chn|\n|    4|   Diya|    73000|     4| Finance|     4|     kkt|\n|    5|  Anaya|    76000|  null| unknown|  null| unknown|\n|    6|  Arjun|    69000|     4| Finance|     3|     chn|\n|    7|   Myra|    74000|     2|      IT|     1|     hyd|\n|    7|Shaurya|    70000|     3|   Sales|  null| unknown|\n|    8|  Meera|    72000|     1|      HR|     4|     kkt|\n|    8|  Yuvan|    67000|  null| unknown|     4|     kkt|\n|    9|  Kiara|    71000|     2|      IT|     3|     chn|\n|   10| Saanvi|    76000|     1|      HR|     4|     kkt|\n|   10|  Rohan|    73000|     1|      HR|  null| unknown|\n+-----+-------+---------+------+--------+------+--------+\n\n"}], "source": "left_df = (\n    emp_df\n    .join(dept_df, emp_df.deptId == dept_df.deptId, \"left\")\n    .join(address_df, emp_df.cityId == address_df.cityId, \"left\")\n    .drop(emp_df.deptId) #you can drop the deptId & cityId column\n    .drop(emp_df.cityId)\n    .dropDuplicates()\n    # .dropna()  # Alternative: drop rows with nulls\n    .fillna(\"unknown\")  # Fill null values with \"unknown\"\n)\n# left_df.show()\nleft_df.orderBy(\"empId\").show()"}, {"cell_type": "code", "execution_count": 69, "id": "06c0be52-e933-4dd7-a722-24a3b45e8614", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+-------+---------+------+--------+------+--------+\n|empId|empName|empSalary|deptId|deptName|cityId|cityName|\n+-----+-------+---------+------+--------+------+--------+\n|    7|Shaurya|    70000|     3|   Sales|  null| unknown|\n|    5|  Anaya|    76000|     0| unknown|  null| unknown|\n|    1|  Aarav|    70000|     1|      HR|     1|     hyd|\n|    3|Reyansh|    68000|     3|   Sales|     2|     blr|\n|    6|  Arjun|    69000|     4| Finance|     3|     chn|\n|    4|   Diya|    73000|     4| Finance|     4|     kkt|\n|    8|  Meera|    72000|     1|      HR|     4|     kkt|\n|    7|   Myra|    74000|     2|      IT|     1|     hyd|\n|    9|  Kiara|    71000|     2|      IT|     3|     chn|\n|    2| Vivaan|    72000|     2|      IT|     2|     blr|\n|    2| Aadhya|    75000|     2|      IT|     2|     blr|\n|    3| Ishaan|    71000|     3|   Sales|     3|     chn|\n|   10| Saanvi|    76000|     1|      HR|     4|     kkt|\n|    8|  Yuvan|    67000|     0| unknown|     4|     kkt|\n|   10|  Rohan|    73000|     1|      HR|  null| unknown|\n+-----+-------+---------+------+--------+------+--------+\n\n"}], "source": "# to handel null values using fillna()\n\nleft_df.fillna(0, \"deptId\").show() # fill the null value as zero\n\n# left_df.fillna(0, \"deptId\").fillna(\"missing\", \"deptName\").show() # fillna"}, {"cell_type": "code", "execution_count": 72, "id": "8f4f4af0-bd9f-472a-addd-5a02127959af", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+----+------+------------------------------------+\n|Id |name|gender|projectDetails                      |\n+---+----+------+------------------------------------+\n|1  |john|male  |[{1, projectA, 6}, {2, projectB, 8}]|\n+---+----+------+------------------------------------+\nonly showing top 1 row\n\n"}], "source": "# sample data\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n\n# spark = SparkSession.builder.appName(\"NestedSchemaExample\").getOrCreate()\n\n# Data\ndata = [\n    (1, 'john', 'male', [\n        (1, \"projectA\", 6),\n        (2, \"projectB\", 8)\n    ]),\n    (2, 'alex', 'male', [\n        (10, \"projectC\", 6),\n        (20, \"projectD\", 8)\n    ])\n]\n\n# Schema\nschema = StructType([\n    StructField(\"Id\", IntegerType(), True),\n    StructField(\"name\", StringType(), True),\n    StructField(\"gender\", StringType(), True),\n    StructField(\"projectDetails\", ArrayType(\n        StructType([\n            StructField(\"projectId\", IntegerType(), True),\n            StructField(\"projectName\", StringType(), True),\n            StructField(\"projectDuration\", IntegerType(), True)\n        ])\n    ), True)\n])\n\n# Create DataFrame\ndf = spark.createDataFrame(data, schema)\n\n# Show DataFrame\ndf.show(1, truncate=False)\n# df.show()\n# df.printSchema()\n"}, {"cell_type": "code", "execution_count": null, "id": "05beb032-02f2-4aac-a0aa-3ef1c524e206", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}