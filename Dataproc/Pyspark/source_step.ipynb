{"cells": [{"cell_type": "code", "execution_count": 1, "id": "feae8269-70a0-4f76-8645-8d5db146777a", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Requirement already satisfied: pyspark in /usr/lib/spark/python (3.1.3)\nRequirement already satisfied: py4j==0.10.9 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pyspark) (0.10.9)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m"}], "source": "# install pysaprk\n\n! pip install pyspark"}, {"cell_type": "code", "execution_count": 2, "id": "15b2efba-dd90-4b82-9c1f-70f99f4817c6", "metadata": {}, "outputs": [{"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://my-cluster-demo1-m.us-east1-c.c.new-gcp-cloud-sql-project.internal:40577\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.3</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7f5d45574e50>"}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}], "source": "spark    "}, {"cell_type": "code", "execution_count": 1, "id": "c9c04238-d6d6-47c4-8037-e96b4c264c2a", "metadata": {}, "outputs": [{"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://my-cluster-demo1-m.us-east1-c.c.new-gcp-cloud-sql-project.internal:34897\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.3</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7fe60e350e20>"}, "execution_count": 1, "metadata": {}, "output_type": "execute_result"}], "source": "# create spark session\n\nimport pyspark\nfrom pyspark.sql import SparkSession\n\nspark = (\n    SparkSession.builder\n    .master(\"local\")\n    .appName(\"source_step\")\n    .getOrCreate()\n)\n\nspark"}, {"cell_type": "code", "execution_count": 4, "id": "a6d4c004-7677-4446-b6aa-a42c24909953", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+------+\n| id|  name|\n+---+------+\n|  1|  anil|\n|  2|sadeep|\n|  3|  john|\n+---+------+\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n\n"}], "source": "# create df from handcoded data\n\nemp = [(1, 'anil'),\n       (2, 'sadeep'),\n       (3, 'john')]\n\ncolumns = ['id', 'name']\n\ndf1 = spark.createDataFrame(data=emp, schema=columns)\ndf1.show()\ndf1.printSchema()"}, {"cell_type": "code", "execution_count": 9, "id": "1a53ae8d-bbde-4fc5-8a9d-f48a14f0338b", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+-------------+---------+----------+--------+\n|customer_id|purchase_date|item_name|item_price|quantity|\n+-----------+-------------+---------+----------+--------+\n|          1|   2022-01-01|   iteam1|      10.0|       2|\n|          2|   2022-01-02|   iteam2|      20.0|       1|\n|          1|   2022-01-03|   iteam3|       5.0|       3|\n|          3|   2022-01-04|   iteam1|      10.0|       1|\n|          1|   2022-01-05|   iteam2|      20.0|       2|\n|          2|   2022-01-06|   iteam3|       5.0|       1|\n+-----------+-------------+---------+----------+--------+\n\nroot\n |-- customer_id: integer (nullable = true)\n |-- purchase_date: string (nullable = true)\n |-- item_name: string (nullable = true)\n |-- item_price: double (nullable = true)\n |-- quantity: integer (nullable = true)\n\n"}], "source": "# create df from csv file\n\nCSV_SOURCE_PATH = \"gs://landing_dataset/sampledata.csv\"\n\ndf2 = spark.read.csv(CSV_SOURCE_PATH, header=True, inferSchema=True)\n\ndf2.show()\ndf2.printSchema()"}, {"cell_type": "code", "execution_count": 10, "id": "36977c3c-606d-4a5b-9509-db8a1d024f17", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------------+----+-----+\n|_corrupt_record| age| name|\n+---------------+----+-----+\n|              [|null| null|\n|           null|  25|Aarav|\n|           null|  30| Isha|\n|           null|  28|Kabir|\n+---------------+----+-----+\n\nroot\n |-- _corrupt_record: string (nullable = true)\n |-- age: long (nullable = true)\n |-- name: string (nullable = true)\n\n"}], "source": "# create df from json file\n\nJSON_SOURCE_PATH = \"gs://landing_dataset/employee_data.json\"\n\ndf3 = spark.read.json(JSON_SOURCE_PATH)\n\ndf3.show()\ndf3.printSchema()"}, {"cell_type": "code", "execution_count": 12, "id": "6ca77073-f3d1-40da-9762-05352df3f586", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+---------------+----------+--------+------------+--------------------+\n|employee_id|           name|department|  salary|joining_date|               email|\n+-----------+---------------+----------+--------+------------+--------------------+\n|       1000|Angelica Cortez| Marketing|74550.95|  2020-07-25|    nlyons@gmail.com|\n|       1001|  Douglas Allen| Marketing|88822.53|  2022-05-06|whitneycarol@wils...|\n+-----------+---------------+----------+--------+------------+--------------------+\nonly showing top 2 rows\n\nroot\n |-- employee_id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: double (nullable = true)\n |-- joining_date: string (nullable = true)\n |-- email: string (nullable = true)\n\n"}], "source": "# create df from parquet file\n\nPARQUET_SOURCE_PATH = \"gs://landing_dataset/userdata2.parquet\"\n\ndf4 = spark.read.parquet(PARQUET_SOURCE_PATH)\n\ndf4.show(2)\ndf4.printSchema()"}, {"cell_type": "code", "execution_count": null, "id": "99ff6818-fea4-4b15-a964-5c6e35699911", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}